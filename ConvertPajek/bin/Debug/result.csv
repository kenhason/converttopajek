A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting|||1996|2935|Yoav Freund
Robert E. Schapire
|http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=986ECCEE224D4DBB854A2D5D9D14AAC2?cid=27811
Instance-based learning algorithms|Abstract. Storing and using specific instances improves the performance of several supervised learning algorithms. These include algorithms that learn decision trees, classification rules, and distributed networks. However, no investigation has analyzed algorithms that use only specific instances to solve incremental learning tasks. In this paper, we describe a framework and methodology, called instance-based learning, that generates classification predictions using only specific instances. Instance-based learning algorithms do not maintain a set of abstractions derived from specific instances. This approach extends the nearest neighbor algorithm, which has large storage requirements. We describe how storage requirements can be significantly reduced with, at most, minor sacrifices in learning rate and classification accuracy. While the storage-reducing algorithm performs well on several realworld databases, its performance degrades rapidly with the level of attribute noise in training instances. Therefore, we extended it with a significance test to distinguish noisy instances. This extended algorithm&#039;s performance degrades gracefully with increasing noise levels and compares favorably with a noise-tolerant decision tree algorithm.|Machine Learning|1991|1264|David W. Aha
Dennis Kibler
Marc K. Albert
|http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=986ECCEE224D4DBB854A2D5D9D14AAC2?cid=20839
Stacked generalization|Abstract: This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-vali-dation’s crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular ques-tion. After introducing stacked generalization and justifying its use, this paper presents two numer-ical experiments. The first demonstrates how stacked generalization improves upon a set of sepa-rate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other ex-perimental evidence in the literature, the usual arguments supporting cross-validation, and the ab-stract justifications presented in this paper, the conclusion is that for almost any real-world gener-alization problem one should use some version of stacked generalization to minimize the general-ization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory. Key Words: generalization and induction, combining generalizers, learning set pre-processing, cross-validation, error estimation and correction.|Neural Networks|1992|651|David H. Wolpert
|http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=986ECCEE224D4DBB854A2D5D9D14AAC2?cid=10759
BoosTexter: A Boosting-based System for Text Categorization|This work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. Our approach is based on a new and improved family of boosting algorithms. We describe in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks. We present results comparing the performance of BoosTexter and a number of other text-categorization algorithms on a variety of tasks. We conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses. |MACHINE LEARNING|2000|600|Robert E. Schapire
Yoram Singer
|http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=986ECCEE224D4DBB854A2D5D9D14AAC2?cid=35520
RCV1: A new benchmark collection for text categorization research|Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection’s properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as|JOURNAL OF MACHINE LEARNING RESEARCH|2004|581|David D. Lewis
Yiming Yang
Tony G. Rose
Fan Li
|http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=986ECCEE224D4DBB854A2D5D9D14AAC2?cid=114322
Estimating Continuous Distributions in Bayesian Classifiers|When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Mateo, 1995 1 Introduction In rec...|In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence|1995|409|George John
Pat Langley
|http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=986ECCEE224D4DBB854A2D5D9D14AAC2?cid=20844
Learning multi-label scene classification|In classic pattern recognition problems, classes are mutually exclusive by definition. Classification errors occur when the classes overlap in the feature space. We examine a different situation, occurring when the classes are, by definition, not mutually exclusive. Such problems arise in semantic scene and document classification and in medical diagnosis. We present a framework to handle such problems and apply it to the problem of semantic scene classification, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels (e.g., a eld scene with a mountain in the background). Such a problem poses challenges to the classic pattern recognition paradigm and demands a different treatment. We discuss approaches for training and testing in this scenario and introduce new metrics for evaluating individual examples, class recall and precision, and overall accuracy. Experiments show that our methods are suitable for scene classification; furthermore, our work appears to generalize to other classification problems of the same nature.||2004|176|Matthew R. Boutell
Jiebo Luo
Xipeng Shen
Christopher M. Brown
|http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=986ECCEE224D4DBB854A2D5D9D14AAC2?cid=400692
Multi-label text classification with a mixture model trained by EM|In many important document classification tasks, documents may each be associated with multiple class labels. This paper describes a Bayesian classification approach in which the multiple classes that comprise a document are represented by a mixture model. While the labeled training data indicates which classes were responsible for generating a document, it does not indicate which class was responsible for generating each word. Thus we use EM to fill in this missing value, learning both the distribution over mixture weights and the word distribution in each class&#039;s mixture component. We describe the benefits of this model and present preliminary results with the Reuters-21578 data set.  |AAAI 99 Workshop on Text Learning|1999|163|Andrew Kachites McCallum
|http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=986ECCEE224D4DBB854A2D5D9D14AAC2?cid=19754
Discriminative Methods for Multi-Labeled Classification|In this paper we present methods of enhancing existing discriminative  classifiers for multi-labeled predictions. Discriminative methods  like support vector machines perform very well for uni-labeled text  classification tasks. Multi-labeled classification is a harder task subject  to relatively less attention. In the multi-labeled setting, classes are often  related to each other or part of a is-a hierarchy. We present a new technique  for combining text features and features indicating relationships  between classes, which can be used with any discriminative algorithm.|In Proceedings of the 8th Pacific-Asia Conference on Knowledge Discovery and Data Mining|2004|83|Shantanu Godbole
Sunita Sarawagi
|http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=986ECCEE224D4DBB854A2D5D9D14AAC2?cid=35519
Protein Classification with Multiple Algorithms|Abstract. Nowadays, the number of protein sequences being stored in central protein databases from labs all over the world is constantly increasing. From these proteins only a fraction has been experimentally analyzed in order to detect their structure and hence their function in the corresponding organism. The reason is that experimental determination of structure is labor-intensive and quite time-consuming. Therefore there is the need for automated tools that can classify new proteins to structural families. This paper presents a comparative evaluation of several algorithms that learn such classification models from data concerning patterns of proteins with known structure. In addition, several approaches that combine multiple learning algorithms to increase the accuracy of predictions are evaluated. The results of the experiments provide insights that can help biologists and computer scientists design high-performance protein classification systems of high quality. 1|10 th Panhelllenic Conference on Informatics (PCI 2005), P. Bozanis and E.N. Houstis (Eds.), Spring-Verlag, LNCS 3746|2005|22|Sotiris Diplaris
Grigorios Tsoumakas
Pericles A. Mitkas
Ioannis Vlahavas
|http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=986ECCEE224D4DBB854A2D5D9D14AAC2?cid=252350
